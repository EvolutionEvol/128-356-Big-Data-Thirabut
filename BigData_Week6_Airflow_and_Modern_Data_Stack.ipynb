{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e44c29dd",
   "metadata": {},
   "source": [
    "# สัปดาห์ที่ 6: ระบบควบคุมเวิร์กโฟลว์ข้อมูล (Orchestration Systems) และ Modern Data Stack\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/witsarutsarai12-Academic/128-356-Big-Data/blob/main/BigData_Week12_Airflow_and_Modern_Data_Stack.ipynb)\n",
    "\n",
    "![AirflowLogo](./images/AirflowLogo.png)\n",
    "\n",
    "ในสัปดาห์นี้ เราจะเจาะลึกเกี่ยวกับการทำงานอัตโนมัติของ Data Pipeline ทำความรู้จักวิวัฒนาการตั้งแต่ยุคที่ไม่มีเครื่องมือควบคุม (Orchestrator) ไปจนถึงอดีตเครื่องมือดังในตลาด และทำไม **Apache Airflow** จึงกลายเป็นมาตรฐานของวงการ นอกจากนี้ เราจะพาไปทำความรู้จักกับเครื่องมือในยุค **Modern Data Stack** (dbt, Airbyte, Snowflake, Databricks) รวมถึงแนวคิดการทำ CI/CD (Continuous Integration / Continuous Deployment) สำหรับ Data Pipeline\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1a0b7b",
   "metadata": {},
   "source": [
    "## 1. Data Pipeline แบบเจาะลึก (Data Pipeline In-Depth)\n",
    "\n",
    "**Data Pipeline** ไม่ใช่แค่การเขียนสคริปต์ดึงข้อมูลเพียงครั้งเดียว แต่เป็นระบบส่งต่อข้อมูลที่ต้องทำงานอย่างต่อเนื่องและเป็นอัตโนมัติ (Automated Ecosystem)\n",
    "\n",
    "### วงจรชีวิตของข้อมูล (Data Lifecycle)\n",
    "1. **Ingestion (Extract)**: การดึงข้อมูลจากแหล่งต้นทาง (Databases, Third-party APIs, File Servers, Logs)\n",
    "2. **Storage (Load)**: การนำข้อมูลดิบไปจัดเก็บรวมไว้ที่ศูนย์กลาง เช่น Data Lake (S3, GCS) หรือ Data Warehouse\n",
    "3. **Processing (Transform)**: การทำความสะอาด (Clean), กรอง (Filter), เชื่อมโยง (Join), และคำนวณสถิติ (Aggregate) ให้ได้ตารางข้อมูลที่พร้อมใช้\n",
    "4. **Serving (Analytics/ML)**: นำข้อมูลที่ผ่านการประมวลผลไปสร้าง Dashboard (BI Tools) หรือใช้ Train Machine Learning Models\n",
    "\n",
    "![Nanobanana Data Lifecycle](./images/nanobanana_data_lifecycle.png)\n",
    "*(ภาพสี่ขั้นตอน Data Lifecycle พาทัวร์โรงงานข้อมูลโดย Nanobanana)*\n",
    "\n",
    "### ความท้าทายของ Data Pipeline ในโลกจริง\n",
    "เมื่อข้อมูลโตขึ้นและระบบซับซ้อนขึ้น เราจะพบว่าสคริปต์ธรรมดาเริ่มเอาไม่อยู่ เพราะ:\n",
    "- **Dependency (ความสัมพันธ์ของงาน)**: งาน B ต้องรอ งาน A เสร็จก่อน ถ้า A ล้มเหลว B ต้องหยุดทำงานชั่วคราว\n",
    "- **Retry & Recovery (การซ่อมแซมและคืนชีพ)**: ถ้าระบบต้นทางล่มชั่วคราว (เช่น API limit, Network ร่วง) ระบบควรจะ Retry ให้เอง ไม่ใช่พังแล้วแช่แข็ง\n",
    "- **Monitoring & Alerting (การแจ้งเตือน)**: ถ้างานล้มเหลว Data Engineer ต้องรับรู้ผ่าน Slack/Email ทันที ไม่ใช่รอจน User มาบอกว่าตัวเลขผิด\n",
    "- **Backfilling (การรันย้อนหลัง)**: ระบบสามารถรันงานของช่วง 1 ปีที่แล้วใหม่ได้ง่ายๆ เมื่อมีการแก้ Logic ของ Code โดยไม่ต้องเขียนสคริปต์ใหม่\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new1_before_cron",
   "metadata": {},
   "source": [
    "## 2. ยุคก่อนมี Orchestrator: โลกที่ไร้ไม้คทาของวาทยกร\n",
    "\n",
    "![Nanobanana Spaghetti Code](./images/nanobanana_spaghetti_cron.png)\n",
    "*(ฝันร้ายของ Nanobanana เมื่อต้องสู้กับ Cron Jobs และ Spaghetti Code)*\n",
    "\n",
    "\n",
    "ลองจินตนาการถึงองค์กรในยุคแรกเริ่มที่เพิ่งเริ่มทำ Data แนวคิดต่างๆ ยังใหม่มาก Data Pipeline มักเริ่มต้นจาก:\n",
    "1. **Manual Run (พลังของสคริปต์บนเครื่องส่วนตัว)**: มี Analyst คนหนึ่ง (มักชื่อสมชาย) เขียน Python Script ไว้ในเครื่อง Laptop สมชายจะต้องตื่นมาตอน 8:00 น. เพื่อกดปุ่ม Run สคริปต์ชุดที่ 1 พอสคริปต์แรกรันเสร็จ ก็จะกดคลิกสคริปต์ชุดที่ 2 ถัดไป ถ้าระหว่างรัน อินเทอร์เน็ตพัง หรือสมชายลาป่วย ข้อมูลวันนั้นก็จะหายไปเลย!\n",
    "2. **Trigger ทับ Trigger (มหากาพย์แห่ง Event แบบสปาเก็ตตี้)**: บริษัทพยายามอัปเกรดระบบ โดยเขียน Logic ไว้ว่า \"เมื่อไฟล์ A ถูกดรอปลงในโฟลเดอร์ ให้เรียกใช้งาน Script B\" และ \"เมื่อ Script B จบ ให้เรียกใช้งาน Script C\" ผลลัพธ์ที่ได้คือโค้ดพันกันยุ่งเหยิง (Spaghetti Code) ไม่มีใครรู้ภาพรวม (Global view) ว่า Pipeline จริงๆ ทำงานอย่างไร และถ้าพังกลางทางตรง B การจะสั่งเริ่มใหม่เฉพาะ C ก็กลายเป็นฝันร้าย\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764b44ff",
   "metadata": {},
   "source": [
    "## 3. ยุคแห่งการตั้งเวลา (Cron Job) และข้อจำกัด\n",
    "\n",
    "ในยุคถัดมา วิศวกรซอฟต์แวร์มักใช้ **Cron Job** (เครื่องมือตั้งเวลาการทำงานในระบบปฏิบัติการตระกูล Unix/Linux) เพื่อสั่งรันทาสก์ต่างๆ แบบตั้งเวลา เช่น:\n",
    "\n",
    "`0 2 * * * /usr/bin/python3 run_extract.py` (ดึงข้อมูลตอนตี 2 ทุกวัน)\n",
    "`0 3 * * * /usr/bin/python3 run_transform.py` (ประมวลผลข้อมูลตอนตี 3 ทุกวัน)\n",
    "\n",
    "### ปัญหาของ Cron Job สำหรับ Data Pipeline อาชีพ\n",
    "1. **ไม่รู้จักความสัมพันธ์ (No Dependency Management)** \n",
    "   - ตั้งไว้ว่ารัน Extract ตี 2, Transform ตี 3... แต่ถ้าวันนึง Extract ข้อมูลใหญ่มาก ใช้เวลาเกิน 1 ชั่วโมง (รันถึงตี 3 ครึ่ง) งาน Transform ที่ตั้งไว้ตี 3 ก็จะเริ่มรันไปพร้อมกัน ทำให้ประมวลผลข้อมูลที่ยังมาไม่ครบ (Data Inconsistency) หรือพัง (Crash)!\n",
    "2. **ทำงานแบบปิดตา (No UI & Hard to Monitor)**\n",
    "   - Cron ทำงานแบบ \"เงียบพังเงียบ\" (Silently fails) ถ้าคุณไม่เขียนโค้ดแจ้งเตือนเอง คุณจะไม่รู้เลยว่ามันพัง การจะดูว่าพังตรงไหนต้อง SSH เข้าไปไล่อ่านไฟล์ Log ยาวๆ เท่านั้น\n",
    "3. **ขาดระบบจัดการความล้มเหลว (Rigid Failure Handling)**\n",
    "   - หาก API ต้นทาง Timeout ชั่วคราว 1 วินาที Cron ไม่รู้จักคำว่า \"รอ 5 นาทีแล้วลองใหม่\" (Retry mechanism) ต้องจัดการเองในโค้ด\n",
    "4. **วุ่นวายเมื่อต้องการประมวลผลย้อนหลัง (No Backfill support)**\n",
    "   - หากต้องการรันข้อมูลย้อนหลัง 15 วัน ต้องเข้าไปแก้โค้ดเพื่อส่ง Parameter วันที่ใหม่ทีละวันๆ\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new2_history_orches",
   "metadata": {},
   "source": [
    "## 4. วิวัฒนาการของเครื่องมือ Data Orchestrator (ก่อนยุค Airflow)\n",
    "\n",
    "จากความวุ่นวายของวัฏจักร Cron โลกของ Data ผ่านวิวัฒนาการหลายยุค โดยแบ่งเป็นฝ่าย Enterprise ยุคเก่า และกลุ่ม Tech Giants:\n",
    "\n",
    "### 4.1 ยุคเครื่องมือระดับลูกพี่ใหญ่ (Enterprise Tools - ยุค 1990s ถึง 2000s)\n",
    "ก่อนยุค Cloud บรรดาธนาคารหรือองค์กรขนาดใหญ่มักใช้เครื่องมือระดับองค์กรที่มีราคาสูง เน้นการทำงานผ่านหน้าจอ UI แบบ **Drag-and-Drop (ลากวางกล่อง)** ยี่ห้อดังในยุคนี้ได้แก่:\n",
    "1. **Informatica (PowerCenter)**: \n",
    "   - ตัวท็อปแห่งยุค ETL แบบ On-premise ราคามหาศาล เก่งมากเรื่องการ Transform ข้อมูลที่ซับซ้อนและการตรวจสอบคุณภาพข้อมูลระดับลึก\n",
    "   - มีหน้าต่าง Workflow Manager เพื่อกดโยงเส้นลำดับกล่องการทำงาน\n",
    "   - **ข้อจำกัด**: เป็นระบบปิด (Proprietary) ขยายตัวยาก (Scale ตามจำนวนเซิร์ฟเวอร์แบบเฉพาะเจาะจง) การเขียนโค้ด Custom เพิ่มทำได้ยากกว่ายุคปัจจุบัน\n",
    "2. **Teradata**: \n",
    "   - ราชาแห่ง Data Warehouse ระดับองค์กร มาในรูปแบบ \"ตู้เหล็กยักษ์\" ผสมฮาร์ดแวร์และซอฟต์แวร์\n",
    "   - มีเครื่องมือ (Utilities) ประจำตัวของมันที่ช่วยกวาดข้อมูลเข้าแบบขนาน (FastLoad, MultiLoad, BTEQ) ทรงพลังอย่างบ้าคลั่งในยุคที่ Cloud ยังไม่เกิด\n",
    "   - **ข้อจำกัด**: ค่าใช้จ่ายแพงระยับ และมักจะผูกพันกับระบบของ Teradata เองเป็นหลัก รวมถึงไม่ได้เกิดมาเพื่อเจาะจงเป็นตัว Orchestrate กลางของเครื่องมืออื่น ๆ นอกระบบนัก\n",
    "3. **IBM DataStage / SAP Data Services**: \n",
    "   - คู่แข่งของ Informatica ในระดับ Enterprise ที่เน้นการลากเส้นเชื่อมกล่องข้อมูลเข้าด้วยกัน\n",
    "\n",
    "**สาเหตุที่เสื่อมความนิยม (สำหรับตัว Orchestrate กลาง):** เมื่อเข้าถึงยุค Big Data ที่ข้อมูลมีความหลากหลายและใหญ่ระดับ Petabyte เครื่องมือสำเร็จรูปราคาแพงที่รันบนเซิร์ฟเวอร์เดี่ยว (Scale-up) ไม่สามารถต้านทานไหว การเขียน Pipeline As Code จึงกลายเป็นทางเลือกใหม่\n",
    "\n",
    "### 4.2 ยุค Open-source จากเหล่า Tech Giants (ยุค 2010s)\n",
    "บรรดาบริษัทไอทีชั้นนำหนีจากโซลูชันเดิม พยายามสร้างระบบตัวเอง และปล่อยเป็น Open-source ให้ชุมชนใช้:\n",
    "1. **Apache Oozie (เกิดจากยุค Hadoop)**: \n",
    "   - อดีตรุ่นพี่ผู้บุกเบิก เกิดมาเพื่อคุมคิวงานใน Hadoop Ecosystem (MapReduce, Hive)\n",
    "   - **ปัญหา**: เขึยน Configuration ด้วยภาษา XML!! ซึ่งเยิ่นเย้อ ช้า ซับซ้อนมาก และขยายตัวยาก เมื่อโลกมุ่งหน้าทิ้ง Hadoop ตัว Oozie จึงตายตามไปด้วย\n",
    "2. **Luigi (พัฒนาโดย Spotify)**: \n",
    "   - ยุโรปเปิดรับยุค *\"Pipeline as Code\"* ด้วยภาษา Python เป็นระบบที่จัดการ Dependency ได้ดี\n",
    "   - **ปัญหา**: ไม่มี Scheduler (มีแค่ Trigger) และ UI ยังอ่อน จึงไม่ครบเครื่องรอบด้าน\n",
    "3. **Azkaban (พัฒนาโดย LinkedIn)**: \n",
    "   - ระบบที่เน้น UI สวยงาม รองรับผู้ใช้หลายคนจัดการและกดรันได้ง่าย\n",
    "   - **ปัญหา**: เขียนเป็นไฟล์ตั้งค่า (Properties file) ไม่ได้รับความยืดหยุ่น 100% จากการลงลึกถึงชั้น Source code แบบ Python\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fc6b3d",
   "metadata": {},
   "source": [
    "## 5. เจาะลึก Apache Airflow (The Engine of Data Pipelines)\n",
    "\n",
    "เมื่อพบข้อจำกัดของบรรพบุรุษ **Maxime Beauchemin** (วิศวกรที่ Airbnb) จึงได้พัฒนา **Apache Airflow** ขึ้นมาในปี 2014 โดยรวมข้อดีคือการเขียนเป็นโค้ด Python (Pipeline as Python Code เหมือน Luigi) รวมกับระบบตั้งเวลา (Scheduler) ที่ทรงพลังและ UI ที่ครบเครื่องที่สุด จนกลายมาเป็นมาตรฐานอันดับหนึ่ง (Standard tool) ของวงการ Data Engineering\n",
    "\n",
    "### แนวคิดหลัก: DAG (Directed Acyclic Graph)\n",
    "แทนที่จะมองการรันโปรแกรมเป็นเส้นตรง Airflow รันงานเป็น **DAG (กราฟระบุทิศทางที่ไม่วนกลับ)**\n",
    "- **Directed (มีทิศทาง)**: รู้ลำดับก่อนหลังชัดเจน งาน A ต้องเสร็จก่อน ถึงจะทำงาน B และ C ต่อได้\n",
    "- **Acyclic (ไม่วนลูป)**: ลูกศรชี้ไปข้างหน้าเสมอ ไม่มีการวกกลับมาที่ Task เดิม เพื่อป้องกันไม่ให้เกิดลูปอนันต์ (Infinite loop)\n",
    "- **Graph (กราฟ)**: วาดโครงสร้างงานแต่ละชิ้นเป็น Node (เรียกว่า Task) เชื่อมการไหลด้วยภาพ\n",
    "\n",
    "การจัดความสัมพันธ์ (Dependencies) ใน Airflow ง่ายมาก เพียงเขียน:\n",
    "`task_a >> [task_b, task_c] >> task_d`\n",
    "\n",
    "![Nanobanana DAG Example](./images/airflow_dag_nanobanana.png)\n",
    "*(ภาพตัวอย่าง DAG ขับร้องโดย Nanobanana ผู้อำนวยเพลง)*\n",
    "\n",
    "---\n",
    "\n",
    "### เจาะลึกสถาปัตยกรรม (Airflow Core Components)\n",
    "Airflow ถูกออกแบบมาเป็น Distributed System มีส่วนประกอบสำคัญดังนี้:\n",
    "\n",
    "1. **Web Server**: ส่วนที่คอยให้บริการ User Interface (UI) แก่ผู้ใช้ เพื่อให้เราสามารถกดเข้าไปดูสถานะการทำงาน (เขียว=สำเร็จ, แดง=ล้มเหลว) กดดู Logs หรือสั่ง Clear Task ให้รันใหม่ได้อย่างง่ายดาย\n",
    "2. **Scheduler (สมองกล)**: คอยตรวจสอบและอ่านไฟล์ DAG ทั้งหมดอย่างต่อเนื่อง เพื่อดูว่ามี Task ไหนถึงเวลารันหรือยัง และ Task ก่อนหน้าเสร็จสมบูรณ์ไหม ถ้าเข้าเงื่อนไข จะส่งงานเข้าไปที่ Executor (ผ่าน Queue)\n",
    "3. **Executor (ฝ่ายจัดการ)**: หน้าที่คือกำหนดว่า จะรันงานนั้นในเครื่องรูปแบบใด (เช่น `LocalExecutor` รันในเครื่องเดี่ยวพร้อมกันตามจำนวน CPU thread หรือ `KubernetesExecutor` ที่สร้าง Pod ใหม่ใน Cluster มารันโดยเฉพาะ)\n",
    "4. **Worker (แรงงาน)**: ตัวเครื่อง/กระบวนการที่รับโค้ดและลงมือรันทำงานจริงๆ (Execution)\n",
    "5. **Metadata Database**: ฐานข้อมูล (ปกติเป็น Postgres/MySQL) สำหรับเก็บ \"สถานะทุกสรรพสิ่ง\" (เช่น รันถึงไหนแล้ว, รันกี่ครั้ง, เวลาสำเร็จ, และ Connections ไปยังฐานข้อมูลปลายทาง)\n",
    "\n",
    "![Nanobanana Airflow Architecture](./images/airflow_architecture_nanobanana.png)\n",
    "*(สถาปัตยกรรม Airflow - นาโนบานานากำลังสั่งงานเหล่า Worker)*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "airflow_tutorial_step0",
   "metadata": {},
   "source": [
    "## 6. สอนเขียน Airflow ทีละก้าว (Step-by-Step Tutorial)\n",
    "\n",
    "ในการสร้าง Workflow 1 ตัว (เรียกว่า DAG) ใน Airflow นั้น เราจะต้องเขียนเป็นโค้ด Python โดยมีโครงสร้างหลักๆ 5 ส่วน ดังนี้:\n",
    "1. **Import Modules**: เรียกใช้ไลบรารีของ Airflow และ Operators ที่จำเป็น\n",
    "2. **Default Arguments**: ตั้งค่าพื้นฐานที่ใช้กับทุกงาน (Task) ใน DAG นี้ (เช่น การ Retry, เจ้าของ DAG)\n",
    "3. **Instantiate a DAG**: สร้างตัวแปร DAG ขึ้นมา และกำหนด Schedule ว่าจะให้รันบ่อยแค่ไหน\n",
    "4. **Tasks (Operators)**: สร้างงานแต่ละชิ้น โดยเลือกใช้ Operator ให้ตรงกับงาน (เช่น นำ BashOperator มารันคำสั่ง Terminal)\n",
    "5. **Dependencies**: จับงานทั้งหมดมาโยงเส้นความสัมพันธ์กัน (วาดกราฟ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "airflow_tutorial_step1",
   "metadata": {},
   "source": [
    "### Step 1: Import ไลบรารีที่จำเป็น\n",
    "ทุกไฟล์ใน Airflow ต้องมีการนำเข้าคลาส `DAG` ฝั่งของเนื้องาน เราจะใช้ `Operators` มาช่วยทำ ซึ่งมีหลายตัวมาก แต่วันนี้เราจะยกตัวอย่าง `BashOperator` (รันสคริปต์/คำสั่งบรรทัด) และ `PythonOperator` (รันฟังก์ชันไพธอน)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install apache-airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "airflow_code_step1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ฟังก์ชันพิเศษสำหรับไว้เรียกใช้ด้วย PythonOperator\n",
    "def print_hello():\n",
    "    print(\"Hello from Airflow Python Operator!\")\n",
    "    return \"Success\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "airflow_tutorial_step2",
   "metadata": {},
   "source": [
    "### Step 2: กำหนด Default Arguments\n",
    "มันคือ Dictionary ที่เก็บการตั้งค่าแบบรวมศูนย์ เพื่อให้ไม่ต้องพิมค่าย้ำๆ ในทุกงาน เช่น ถ้ารันพังให้ Retry ได้ 3 ครั้ง (ทิ้งช่วง 5 นาที)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "airflow_code_step2",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    'owner': 'data_engineering_student',    # ระบุชื่อผู้ดูแล DAG\n",
    "    'depends_on_past': False,               # รันงานวันนี้ได้เลย ไม่ต้องรอของเมื่อวานให้สำเร็จก่อน\n",
    "    'email_on_failure': False,              # ไม่ต้องส่งแจ้งเตือนถ้าพัง (ในระบบจริงมักเปิดเป็น True)\n",
    "    'email_on_retry': False,                # ไม่ต้องเตือนถ้ากำลัง Retry\n",
    "    'retries': 2,                           # จำนวนครั้งที่ยอมให้รันใหม่ถ้าล้มเหลว\n",
    "    'retry_delay': timedelta(minutes=5),    # ระยะห่างในการ Retry แต่ละครั้ง\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "airflow_tutorial_step3",
   "metadata": {},
   "source": [
    "### Step 3: ประกาศโครงสร้างร่มใหญ่ของ DAG (Instantiate DAG)\n",
    "เราจะต้องตั้งชื่อ (DAG_ID), ระบุวันเริ่มต้น และรอบการตั้งเวลา (Schedule Interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "airflow_code_step3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with DAG(\n",
    "    dag_id='my_first_airflow_tutorial',       # ชื่อของ DAG ห้ามซ้ำกับตัวอื่นในระบบ\n",
    "    default_args=default_args,                # เรียกใช้การตั้งค่าจาก Step 2\n",
    "    description='A simple tutorial DAG',      # คำอธิบายสั้นๆ (จะไปโชว์ใน UI)\n",
    "    schedule_interval=timedelta(days=1),      # รันทุกๆ 1 วัน (สามารถเขียนเป็น Cron Expression ได้ เช่น '0 0 * * *')\n",
    "    start_date=datetime(2023, 1, 1),          # วันเวลาเริ่มต้นที่ยอมให้ DAG ทำงานได้\n",
    "    catchup=False                             # False = ไม่ต้องไล่รันย้อนหลังตั้งแต่วันที่ 1 ม.ค. ถึงปัจจุบัน (รันข้ามมาวันนี้เลย)\n",
    ") as dag:\n",
    "    # --- พื้นที่สำหรับใส่ Task ใน Step 4 และ 5 จะอยู่เยื้องใน block ถัดไป ---\n",
    "    pass  # ใช้ pass ไปก่อนเพื่อไม่ให้ error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "airflow_tutorial_step4",
   "metadata": {},
   "source": [
    "### Step 4 & 5: สร้างชิ้นงาน (Tasks) และร้อยเรียงทิศทาง (Dependencies)\n",
    "เราจะเรียกใช้ Operators ออกมาสร้างเป็นตัวแปร (เปรียบเสมือน Worker แต่ละคนที่เรารับสมัครเข้ามา) ให้แน่ใจว่าได้ระบุ `task_id` ซึ่งเป็นชื่อประจำตึกของงานนั้น\n",
    "\n",
    "**สุดท้าย (สำคัญที่สุด):** การกำหนด Dependency ด้วยเครื่องหมาย `>>`  (ลูกศรพุ่งไปข้างหน้า)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "airflow_code_step45",
   "metadata": {},
   "outputs": [],
   "source": [
    "with DAG(\n",
    "    dag_id='my_first_airflow_tutorial', \n",
    "    default_args=default_args, \n",
    "    schedule_interval='@daily', \n",
    "    start_date=datetime(2023, 1, 1), \n",
    "    catchup=False\n",
    ") as dag:\n",
    "\n",
    "    # Task A: รันคำสั่ง Print ธรรมดาผ่าน Terminal\n",
    "    job_1 = BashOperator(\n",
    "        task_id='start_greeting',\n",
    "        bash_command='echo \"Starting my awesome pipeline!\"'\n",
    "    )\n",
    "\n",
    "    # Task B: สั่ง Sleep นับถอยหลัง \n",
    "    job_2 = BashOperator(\n",
    "        task_id='wait_for_10_seconds',\n",
    "        bash_command='sleep 10'\n",
    "    )\n",
    "\n",
    "    # Task C: รันฟังก์ชันไพธอนที่เตรียมไว้ด้านบน\n",
    "    job_3 = PythonOperator(\n",
    "        task_id='run_my_python_code',\n",
    "        python_callable=print_hello\n",
    "    )\n",
    "\n",
    "    # Task 4: ขอลองสร้างอีกตัวให้มารันขนานกับ Job 3\n",
    "    job_4_parallel = BashOperator(\n",
    "        task_id='i_am_running_parallel',\n",
    "        bash_command='echo \"This runs at the same time as job 3!\"'\n",
    "    )\n",
    "\n",
    "    # ------ ประกอบส่วน Dependency ------\n",
    "    # ความสัมพันธ์: ให้ 1 รันเสร็จก่อน ค่อยตามด้วย 2\n",
    "    # จากนั้น 3 และ 4 รันพร้อมกันเพราะต่อท้าย 2 เหมือนกัน\n",
    "    \n",
    "    job_1 >> job_2 >> [job_3, job_4_parallel]\n",
    "    \n",
    "    # เพียงเท่านี้ เราก็จะได้กราฟ 1-2-<(3,4)> อย่างสมบูรณ์!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "airflow_tutorial_conclusion",
   "metadata": {},
   "source": [
    "เมื่อนำซอร์สโค้ดนี้ไปวางลงในโฟลเดอร์รันของ Airflow ตัว **Scheduler** ของ Airflow จะตรวจเจอไฟล์นี้โดยอัตโนมัติ ภายใน 1 นาที มันจะไปแสดงผลขึ้นที่ฝั่งหน้าจอ **Web Server UI** เป็นตารางสวยงาม ให้เรากด Monitor ได้ทันที!\n",
    "\n",
    "---\n",
    "### Step 6: จะลองเล่น Airflow ได้อย่างไร? (Installation & Free Alternatives)\n",
    "การจะเซ็ตอัปเครื่องเซิร์ฟเวอร์ Airflow เต็มสูบแบบ Distributed System ต้องใช้ความพยายามสูงมาก (ต้องตั้ง Database, Workers, Webserver) แต่สำหรับผู้เริ่มต้น มี 3 ทางเลือกยอดฮิต:\n",
    "\n",
    "**ทางเลือกที่ 1: ติดตั้งแบบรวบรัดบนเครื่องตัวเอง (Local Docker) - แนะนำที่สุด**\n",
    "Airflow มีไฟล์ `docker-compose.yaml` แจกฟรี เพียงแค่ในเครื่องคุณมีโปรแกรม Docker Desktop ก็ทำตามรันคำสั่งนี้ได้เลย:\n",
    "```bash\n",
    "curl -LfO 'https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml'\n",
    "docker-compose up -d\n",
    "```\n",
    "จากนั้นเข้า Web Browser ไปที่ `http://localhost:8080` (Username/Password: airflow) ก็จะเจอกับหน้าจอ UI ให้กดเล่นได้เลย\n",
    "\n",
    "**ทางเลือกที่ 2: ติดตั้งด้วย Python PIP (เหมาะกับทดสอบเล่นๆ แบบ Standalone)**\n",
    "```bash\n",
    "pip install apache-airflow\n",
    "airflow standalone\n",
    "```\n",
    "คำสั่งนี้จะบีบอัดทุกอย่าง (Web Server, Scheduler, Database แบบย่อ) ลงมาอยู่ใน Terminal บรรทัดเดียว เหมาะสำหรับผู้ริเริ่มเขียน DAG เบื้องต้น\n",
    "\n",
    "**ทางเลือกที่ 3: ใช้บริการ Cloud (Managed Service - ไม่ต้องลงโปรแกรมเอง)**\n",
    "บริษัทส่วนใหญ่มักเช่า Airflow สำเร็จรูป ไม่นิยมติดตั้งเองด้วยคอนเทนเนอร์ เช่น\n",
    "- **Astronomer (Astro)**: แพลตฟอร์มจากบริษัทของกลุ่มผู้สร้าง Airflow มี **Free Trial ให้ทดลองประทับตราฟรี** ล็อกอินและ Deploy Pipeline ขึ้น Cloud ได้เลย เหมาะสำหรับผู้เริ่มต้นที่ไม่อยากเปิด Docker ในคอม\n",
    "- **Amazon MWAA / Google Cloud Composer**: Airflow ที่อินทิเกรตอย่างสมบูรณ์แบบกับ AWS / Google Cloud (มีค่าใช้จ่าย)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "airflow_on_colab",
   "metadata": {},
   "source": [
    "### โบนัส: รัน Airflow UI บน Google Colab (วิธีเสถียรกว่าเดิม)\n",
    "\n",
    "บน Colab เราเปิด `localhost:8080` ตรงๆ ไม่ได้ จึงต้องทำ tunnel ออกมาภายนอก\n",
    "ในเวอร์ชันนี้เปลี่ยนจาก **localtunnel** เป็น **cloudflared** ซึ่งเสถียรกว่าและไม่ต้องกรอกรหัสผ่านหน้าเว็บคั่นกลาง\n",
    "\n",
    "**ขั้นตอนการรันบน Colab (รันทีละ Cell):**\n",
    "1. ติดตั้ง Airflow ด้วย constraints ให้ตรง Python ของ Colab\n",
    "2. ติดตั้ง `cloudflared`\n",
    "3. สตาร์ต `airflow standalone` และรอให้ Web Server พร้อม\n",
    "4. เปิด tunnel และคลิก URL ที่ได้ (`https://...trycloudflare.com`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_airflow_colab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) ติดตั้ง Airflow (ล้างแพ็กเกจเก่าก่อนเพื่อกันชนกันระหว่าง Airflow 2.x/3.x)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "py_ver = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "airflow_ver = \"2.10.5\"\n",
    "constraints_url = f\"https://raw.githubusercontent.com/apache/airflow/constraints-{airflow_ver}/constraints-{py_ver}.txt\"\n",
    "\n",
    "print(f\"Installing Airflow {airflow_ver} for Python {py_ver}\")\n",
    "print(f\"Constraints: {constraints_url}\")\n",
    "\n",
    "# เคลียร์แพ็กเกจ Airflow ที่อาจค้างจากคนละ major version (โดยเฉพาะ Airflow 3 split packages)\n",
    "%pip uninstall -y -q apache-airflow apache-airflow-core apache-airflow-task-sdk apache-airflow-providers-standard\n",
    "\n",
    "# ติดตั้งใหม่แบบ clean\n",
    "%pip install -q --no-cache-dir \"apache-airflow=={airflow_ver}\" --constraint \"{constraints_url}\"\n",
    "\n",
    "# Colab hotfix: บาง runtime มี typing_extensions เก่า ทำให้ pydantic_core import Sentinel ไม่ได้\n",
    "%pip uninstall -y -q typing_extensions\n",
    "%pip install -q --no-cache-dir --force-reinstall \"typing_extensions>=4.12.2\"\n",
    "\n",
    "import importlib\n",
    "import airflow\n",
    "import typing_extensions as te\n",
    "importlib.reload(te)\n",
    "print(\"Airflow:\", airflow.__version__, airflow.__file__)\n",
    "print(\"Kernel typing_extensions:\", te.__file__, getattr(te, \"__version__\", \"unknown\"), \"Sentinel:\", hasattr(te, \"Sentinel\"))\n",
    "\n",
    "# ตรวจใน subprocess ด้วย (airflow command จะรันเป็น subprocess)\n",
    "subprocess.run(\n",
    "    [\n",
    "        sys.executable,\n",
    "        \"-c\",\n",
    "        \"import airflow, typing_extensions as te; print('Subprocess Airflow:', airflow.__version__, airflow.__file__); print('Subprocess typing_extensions:', te.__file__, getattr(te,'__version__','unknown'), 'Sentinel:', hasattr(te,'Sentinel'))\",\n",
    "    ],\n",
    "    check=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init_airflow_colab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) ติดตั้ง Cloudflared เพื่อ tunnel พอร์ต 8080\n",
    "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O /usr/local/bin/cloudflared\n",
    "!chmod +x /usr/local/bin/cloudflared\n",
    "!cloudflared --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_airflow_colab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "import airflow\n",
    "\n",
    "AIRFLOW_HOME = \"/content/airflow\"\n",
    "os.environ[\"AIRFLOW_HOME\"] = AIRFLOW_HOME\n",
    "os.environ[\"AIRFLOW__CORE__LOAD_EXAMPLES\"] = \"False\"\n",
    "os.environ[\"AIRFLOW__WEBSERVER__WORKERS\"] = \"1\"\n",
    "\n",
    "# Force default XCom backend ให้ตรงกับ Airflow major version (กัน config เก่าชนกัน)\n",
    "airflow_major = int(airflow.__version__.split(\".\")[0])\n",
    "if airflow_major >= 3:\n",
    "    os.environ[\"AIRFLOW__CORE__XCOM_BACKEND\"] = \"airflow.sdk.execution_time.xcom.BaseXCom\"\n",
    "else:\n",
    "    os.environ[\"AIRFLOW__CORE__XCOM_BACKEND\"] = \"airflow.models.xcom.BaseXCom\"\n",
    "\n",
    "print(\"Airflow version:\", airflow.__version__)\n",
    "print(\"Using XCom backend:\", os.environ[\"AIRFLOW__CORE__XCOM_BACKEND\"])\n",
    "\n",
    "# กัน process เก่าค้าง\n",
    "os.system(\"pkill -f 'airflow webserver' >/dev/null 2>&1\")\n",
    "os.system(\"pkill -f 'airflow scheduler' >/dev/null 2>&1\")\n",
    "os.system(\"pkill -f 'airflow standalone' >/dev/null 2>&1\")\n",
    "os.system(\"pkill -f 'cloudflared tunnel' >/dev/null 2>&1\")\n",
    "time.sleep(2)\n",
    "\n",
    "# ล้าง AIRFLOW_HOME เก่าที่อาจค้าง state พัง\n",
    "if os.path.exists(AIRFLOW_HOME):\n",
    "    shutil.rmtree(AIRFLOW_HOME)\n",
    "os.makedirs(AIRFLOW_HOME, exist_ok=True)\n",
    "\n",
    "# Fallback patch: ถ้า runtime ยังเจอ typing_extensions เก่า ให้เติม Sentinel ตอนเริ่ม interpreter\n",
    "fix_dir = \"/content/pyfix\"\n",
    "os.makedirs(fix_dir, exist_ok=True)\n",
    "with open(f\"{fix_dir}/sitecustomize.py\", \"w\") as f:\n",
    "    f.write(\n",
    "        \"import typing_extensions as te\\n\"\n",
    "        \"if (not hasattr(te, 'Sentinel')) and hasattr(te, '_Sentinel'):\\n\"\n",
    "        \"    te.Sentinel = te._Sentinel\\n\"\n",
    "    )\n",
    "\n",
    "env = os.environ.copy()\n",
    "env[\"PYTHONPATH\"] = fix_dir + (\":\" + env[\"PYTHONPATH\"] if env.get(\"PYTHONPATH\") else \"\")\n",
    "\n",
    "# sanity check in subprocess ก่อนรัน standalone\n",
    "subprocess.run(\n",
    "    [\n",
    "        sys.executable,\n",
    "        \"-c\",\n",
    "        \"import os, airflow; print('Preflight Airflow:', airflow.__version__, airflow.__file__); print('Preflight XCom backend env:', os.getenv('AIRFLOW__CORE__XCOM_BACKEND'))\",\n",
    "    ],\n",
    "    check=True,\n",
    "    env=env,\n",
    ")\n",
    "\n",
    "print(\"Starting airflow standalone...\")\n",
    "airflow_log = \"/content/airflow_standalone.log\"\n",
    "with open(airflow_log, \"w\") as logf:\n",
    "    proc = subprocess.Popen([\"airflow\", \"standalone\"], stdout=logf, stderr=subprocess.STDOUT, env=env)\n",
    "\n",
    "# รอให้ Airflow เปิด health endpoint (นานขึ้นสำหรับ Colab ที่ช้า)\n",
    "ready = False\n",
    "for i in range(420):\n",
    "    if proc.poll() is not None:\n",
    "        print(\"Airflow process exited before becoming healthy.\")\n",
    "        break\n",
    "    try:\n",
    "        urllib.request.urlopen(\"http://127.0.0.1:8080/health\", timeout=2)\n",
    "        ready = True\n",
    "        print(\"Airflow webserver is ready on :8080\")\n",
    "        break\n",
    "    except Exception:\n",
    "        if i % 20 == 0:\n",
    "            print(f\"Waiting for Airflow... {i}s\")\n",
    "        time.sleep(1)\n",
    "\n",
    "if not ready:\n",
    "    print(\"\\n===== airflow_standalone.log (tail) =====\")\n",
    "    if os.path.exists(airflow_log):\n",
    "        with open(airflow_log) as f:\n",
    "            lines = f.readlines()\n",
    "        print(\"\".join(lines[-150:]))\n",
    "    raise RuntimeError(\"Airflow did not become ready. See log tail above.\")\n",
    "\n",
    "# ดึง username/password ที่ airflow generate ให้\n",
    "password = None\n",
    "username = \"admin\"\n",
    "new_pwd_file = f\"{AIRFLOW_HOME}/simple_auth_manager_passwords.json.generated\"\n",
    "old_pwd_file = f\"{AIRFLOW_HOME}/standalone_admin_password.txt\"\n",
    "\n",
    "if os.path.exists(new_pwd_file):\n",
    "    with open(new_pwd_file) as f:\n",
    "        auth_data = json.load(f)\n",
    "        username = list(auth_data.keys())[0]\n",
    "        password = auth_data[username]\n",
    "elif os.path.exists(old_pwd_file):\n",
    "    with open(old_pwd_file) as f:\n",
    "        content = f.read().strip()\n",
    "        password = content.split(\"Password:\")[-1].strip() if \"Password:\" in content else content\n",
    "\n",
    "print(f\"Username: {username}\")\n",
    "if password:\n",
    "    print(f\"Password: {password}\")\n",
    "else:\n",
    "    print(\"Password file not found yet. You can inspect /content/airflow_standalone.log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lt_airflow_colab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# 4) เปิด Cloudflared tunnel และแสดง URL สำหรับเข้า Airflow UI\n",
    "os.system(\"pkill -f 'cloudflared tunnel' >/dev/null 2>&1\")\n",
    "\n",
    "tunnel_log = \"/content/cloudflared.log\"\n",
    "with open(tunnel_log, \"w\") as logf:\n",
    "    subprocess.Popen(\n",
    "        [\"cloudflared\", \"tunnel\", \"--url\", \"http://127.0.0.1:8080\", \"--no-autoupdate\"],\n",
    "        stdout=logf,\n",
    "        stderr=subprocess.STDOUT,\n",
    "    )\n",
    "\n",
    "public_url = None\n",
    "for _ in range(60):\n",
    "    time.sleep(1)\n",
    "    if os.path.exists(tunnel_log):\n",
    "        with open(tunnel_log) as f:\n",
    "            logs = f.read()\n",
    "        m = re.search(r\"https://[-a-zA-Z0-9]+\\.trycloudflare\\.com\", logs)\n",
    "        if m:\n",
    "            public_url = m.group(0)\n",
    "            break\n",
    "\n",
    "if public_url:\n",
    "    print(\"Open this Airflow UI URL:\")\n",
    "    print(public_url)\n",
    "    print(\"If the URL expires, rerun this cell to create a new tunnel.\")\n",
    "else:\n",
    "    print(\"Tunnel URL not found yet. Run: !tail -n 80 /content/cloudflared.log\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b021d7",
   "metadata": {},
   "source": [
    "## 7. ยุคของ Modern Data Stack (MDS)\n",
    "\n",
    "เมื่อเทคโนโลยี Cloud ก้าวหน้าขึ้น องค์กรต้องการสร้าง Data Pipeline ที่รวดเร็วกว่าเดิม โดยใช้เครื่องมือสำเร็จรูปมากขึ้น ทิศทางของ Pipeline เปลี่ยนจากกรอบ **ETL (Extract -> Transform -> Load)** เดิมแบบ On-premise ที่กระบวนการแปลง (Transform) ต้องใช้เครื่องประมวลผลขนาดใหญ่ตรงกลาง มาสู่แนวคิด **ELT (Extract -> Load -> Transform)**\n",
    "\n",
    "**หลักการของ ELT**: *\"ก็ดูดข้อมูลดิบทั้งหมด (E) ไปกองไว้ใน Cloud Data Warehouse ยักษ์ๆ เลย (L) แล้วค่อยใช้พลังคิวรี (Query) ที่มหาศาลของ Warehouse นั้นๆ ในการแปลงข้อมูล (T)\"* \n",
    "\n",
    "![Modern Data Stack Architecture](./images/modern-data-stack.png)\n",
    "*(ภาพจำลองโครงสร้าง Modern Data Stack ที่มีเครื่องมือใหม่ๆ เข้ามาแทนที่ - ที่มา: Metabase)*\n",
    "\n",
    "Modern Data Stack (MDS) จึงประกอบด้วยเครื่องมือชูโรง ดังนี้:\n",
    "\n",
    "### 7.1. ฝ่ายดึงข้อมูล (The \"Extract & Load\" Phase): Airbyte / Fivetran\n",
    "- ภาระการเขียนสคริปต์ดึง API ของฐานข้อมูลนับ 100 แหล่ง เป็นเรื่องน่าเบื่อและซ้ำซาก (ต้องคอยแก้เวลา API เค้าอัปเดตเวอร์ชัน)\n",
    "- **Airbyte** (Open-source) และ **Fivetran** (Commercial) คือเครื่องมือสำเร็จรูปที่ทำหน้าที่ดึงข้อมูลอย่างเดียว (EL) มี Connector สำเร็จรูปมหาศาล (Facebook Ads, Salesforce, MongoDB ฯลฯ) แค่กดคลิกผ่าน UI ข้อมูลก็จะไหลเข้า Warehouse ปลายทางอย่างต่อเนื่อง\n",
    "\n",
    "### 7.2. ฐานทัพหลัก (The Storage & Compute Engine): Snowflake / Databricks\n",
    "- **Snowflake (Cloud Data Warehouse)**: \n",
    "  - เปลี่ยนวงการด้วยสถาปัตยกรรมที่แยก **Storage** กับ **Compute** ออกจากกันอย่างเด็ดขาด อยากเก็บข้อมูลมหาศาลก็จ่ายค่าเช่ารายเดือนถูกๆ พอๆ กับ S3 (อิงตามพื้นที่) อยากคิวรีรีพอร์ตหนักๆ สิ้นเดือนก็ค่อยเปิดโหนด Compute แรงๆ มาวิเคราะห์ จบแล้วก็ปิดทิ้งได้ \n",
    "  - โดดเด่นด้านการใช้งานแบบ SQL และเหมาะใช้ทำ Data Management เต็มรูปแบบ\n",
    "- **Databricks (Data Lakehouse)**: \n",
    "  - สร้างจากทีมผู้ก่อตั้ง Apache Spark โดยพยายามประสานจุดเด่นของ มาสู่แนวคิด \"Lakehouse\"\n",
    "  - ใช้เป็นหัวใจหลักได้ทั้งงานวิเคราะห์ SQL ทั่วไป, งาน Streaming ระดับมิลลิวินาที และงานจัดเตรียมข้อมูลสร้างโมเดล Machine Learning แบบครบวงจร\n",
    "\n",
    "### 7.3. ฝ่ายแปลงข้อมูล (The \"Transform\" Phase): dbt (data build tool)\n",
    "- จากเดิมที่ Data Engineer ต้องเขียนโค้ดซับซ้อน หรือ Analyst ต้องทำ Scheduled Query เป็นร้อยๆ ตัวทับซ้อนกันใน Warehouse\n",
    "- **dbt (data build tool)** คือฮีโร่ที่เข้ามาทำหน้าที่ตระกูล \"T\" ใน ELT โดยยอมให้เราเขียนแค่คำสั่ง `SELECT` พื้นฐาน แต่อัดฉีดความเป็น Software Engineering เข้าไปลึกสุดขั้ว\n",
    "- **ฟีเจอร์เด่น:**\n",
    "  1. โค้ดถูกผูกกับ Version Control (Git) ใครแก้เมื่อไหร่ทีมรู้หมด\n",
    "  2. วาดกราฟความสัมพันธ์ (Dependency/Data Lineage) ของตารางข้อมูลทั้งหมดให้แบบอัตโนมัติ รู้อดีต รู้อนาคตว่าถ้าทำตาราง A พัง ตาราง B ที่รอใช้จะพังตาม\n",
    "  3. มีระบบ Data Testing แบบ Built-in ช่วยดักจับ Error เชิงโครงสร้างข้อมูลไว้ก่อนรันลง Production จริง (เช่น คอลัมน์ User_ID ห้ามเป็นค่า Null ห้ามมีค่าซ้ำ)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new3_cicd_pipeline",
   "metadata": {},
   "source": [
    "## 8. แถม: CI/CD Pipeline สำหรับ Data Engineering (Automation Beyond Airflow)\n",
    "\n",
    "![Nanobanana CI/CD](./images/nanobanana_cicd.png)\n",
    "*(Nanobanana คุมเครื่องจักรสายพาน CI/CD - ทุกอย่างต้องอัตโนมัติ!)*\n",
    "\n",
    "\n",
    "เราใช้ Airflow จัดการความสัมพันธ์ของ \"Data\"... \n",
    "แต่ในการทำงานของวิศวกรข้อมูล โค้ดที่พวกเขาเขียนขึ้นล่ะ ใครเป็นคนตรวจสอบ และฝังลงไปบน Server? คำตอบคือ **CI/CD Pipelines (Continuous Integration / Continuous Deployment)**\n",
    "\n",
    "แนวคิดนี้ถูกส่งผ่านมาจาก Software Engineering (DevOps) สู่ Data Engineering:\n",
    "- **CI (การรวมโค้ดต่อเนื่อง)**: เมื่อมีนักพัฒนาเขียนโค้ดใหม่ (เช่น เพิ่ม DAG ใน Airflow หรือแก้ dbt model) และทำการ Push ขึ้น Git ระบบจะนำโค้ดนั้นมารัน **Tests** อัตโนมัติ เพื่อเช็คการแตกหัก (Syntax Error / Logic Error)\n",
    "- **CD (การส่งมอบต่อเนื่อง)**: เมื่อผ่านการเทสทั้้งหมด โค้ดใหม่จะถูก นำไปวาง (Deploy) บน Production Server อัตโนมัติโดยที่มนุษย์ไม่ต้องอัพโหลดไฟล์เอง\n",
    "\n",
    "### เครื่องมือชื่อดังในตลาด\n",
    "1. **Jenkins (รุ่นเก๋า ยืดหยุ่นสูงสุด)**: \n",
    "   - เครื่องมือขวัญใจทีม DevOps ยุคบุกเบิก เป็น Automation Server ที่เขียน Pipeline เชิงสคริปต์ได้ทุกอย่าง (อยากให้รัน Shell, ยิง API ทำได้หมด)\n",
    "2. **Argo CD (สาย Kubernetes)**:\n",
    "   - ปัจจุบัน Infrastructures มักวิ่งอยู่บนระบบตู้คอนเทนเนอร์ (Kubernetes) \n",
    "   - Argo CD โดดเด่นด้วยโมเดล *GitOps* คือ \"โค้ดใน Git เขียนอย่างไร, สภาพแวดล้อมจริงบนเซิร์ฟเวอร์ต้องเป็นอย่างนั้นเป๊ะๆ\" ทันทีที่มีการแก้โค้ด Kubernetes จะดึง (Pull) การเปลี่ยนแปลงไปปรับทันทีแบบไร้รอยต่อ\n",
    "3. **Terraform (Infrastructure as Code - IaC)**:\n",
    "   - แทนที่คุณจะเข้าหน้าเว็บ AWS/GCP ไปกดคลิกสร้าง Server 10 เครื่อง (รัน Airflow), สร้าง Database 1 ตัว, สร้าง Cloud Storage... คุณเขียนทั้งหมดเป็นไฟล์ **Terraform Code** (`.tf`) \n",
    "   - พอรัน `terraform apply` มันจะไปคุยกับ Cloud provider สร้าง Server ปลั๊กอิน Network จัดสรร Database ให้ทั้งหมดตามที่พิมพ์ไว้ (ถ้าระบบพัง ก็แค่ใช้โค้ดชุดนี้รันใหม่ ได้บ้านหลังใหม่หน้าตาเหมือนเดิม 100% ภายใน 5 นาที!)\n",
    "\n",
    "**ข้อคิด:** Airflow เป็นไม้คทาของ Data แต่ CI/CD & Terraform คือไม้คทาแห่ง Infrastructure!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Workshop: สร้าง Data Pipeline ของจริงด้วยชุดข้อมูลจริง\n",
    "\n",
    "📝 **โจทย์:** เพื่อให้เห็นภาพการทำงานจริง เราจะสร้าง Pipeline กรองข้อมูลผู้โดยสารเรือ Titanic ยอดฮิต โดยมี 3 ขั้นตอนที่ต้อง**รอให้งานก่อนหน้าทำเสร็จก่อน**:\n",
    "1. **Download Data**: โหลดไฟล์ CSV จากอินเทอร์เน็ต\n",
    "2. **Process Data**: รันสคริปต์ Python แยกต่างหาก (Standalone Job) เพื่อลบแถวที่ข้อมูลหาย และสรุปยอดผู้รอดชีวิต\n",
    "3. **Archive**: ย้ายไฟล์ที่ประมวลผลเสร็จแล้วไปเก็บในโฟลเดอร์ Archive\n",
    "\n",
    "*(แผนผังการทำงาน: `Download` ➜ `Process` ➜ `Archive`)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ขั้นตอนที่ 1: สร้างโฟลเดอร์สำหรับจำลองระบบการทำงาน\n",
    "ก่อนอื่น เราจำลองว่าเรากำลังทำงานอยู่ในเครื่อง Server จริงๆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# สร้างโฟลเดอร์จำลอง\n",
    "os.makedirs(\"my_data_pipeline/raw_data\", exist_ok=True)\n",
    "os.makedirs(\"my_data_pipeline/processed_data\", exist_ok=True)\n",
    "os.makedirs(\"my_data_pipeline/archive\", exist_ok=True)\n",
    "\n",
    "print(\"✅ สร้างโฟลเดอร์สำหรับรับส่งข้อมูลสำเร็จเป้าหมายแล้ว!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ขั้นตอนที่ 2: เขียน Python Job (Standalone) ให้ลูกน้อง (Worker) เอาไปรัน\n",
    "เราจะไม่เอา Code จัดการข้อมูล (Data Transformation) ไปยัดไว้ในไฟล์ Airflow DAG ตรงๆ แต่จะเขียนแยกเป็นไฟล์ `clean_titanic.py` ให้ Airflow เรียกใช้อีกที (Best Practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile my_data_pipeline/clean_titanic.py\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def run_cleaning():\n",
    "    input_file = \"my_data_pipeline/raw_data/titanic.csv\"\n",
    "    output_file = \"my_data_pipeline/processed_data/titanic_cleaned.csv\"\n",
    "    \n",
    "    # 1. เช็คก่อนว่ามีไฟล์ให้ทำไหม (ถ้าไม่มีแปลว่า Task ก่อนหน้าพังหรือไม่ได้รัน)\n",
    "    if not os.path.exists(input_file):\n",
    "        raise FileNotFoundError(f\"❌ ไม่พบไฟล์ {input_file} (Task ดาวน์โหลดอาจจะพัง!)\")\n",
    "\n",
    "    print(\"✅ พบไฟล์ข้อมูลดิบ กำลังเริ่มกระบวนการ Clean...\")\n",
    "    \n",
    "    # 2. อ่านข้อมูล\n",
    "    df = pd.read_csv(input_file)\n",
    "    original_rows = len(df)\n",
    "\n",
    "    # 3. คลีนข้อมูล (Drop แถวที่มีค่า Null ในคอลัมน์ Age)\n",
    "    df_clean = df.dropna(subset=[\"Age\"])\n",
    "    \n",
    "    # 4. เซฟไฟล์ที่เสร็จแล้วลงโฟลเดอร์ปลายทาง\n",
    "    df_clean.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"📊 สรุปผลการ Clean:\")\n",
    "    print(f\"   - ข้อมูลต้นทาง: {original_rows} แถว\")\n",
    "    print(f\"   - ข้อมูลหลัง Clean (ตัด Age ว่างทิ้ง): {len(df_clean)} แถว\")\n",
    "    print(f\"✅ เซฟไฟล์ผลลัพธ์ไปที่ {output_file} เรียบร้อยแล้ว\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_cleaning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ขั้นตอนที่ 3: เอาเชือกมาผูกงานด้วย Airflow DAG (`titanic_pipeline.py`)\n",
    "ทีนี้ก็ถึงคราวพระเอกของเรา (Airflow) ออกโรงจัดระเบียบ 3 งานนี้ให้ทำงานต่อคิวกันอย่างสมบูรณ์แบบ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile my_data_pipeline/titanic_pipeline.py\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"data_student\",\n",
    "    \"retries\": 2, # ถ้าพังให้ลองใหม่ 2 ครั้ง\n",
    "    \"retry_delay\": timedelta(minutes=1),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"titanic_data_processing_pipeline\",\n",
    "    default_args=default_args,\n",
    "    schedule_interval=\"@daily\", \n",
    "    start_date=datetime(2023, 1, 1),\n",
    "    catchup=False\n",
    ") as dag:\n",
    "\n",
    "    # 📌 งานที่ 1: โหลดไฟล์ CSV ตำนานเรือไททานิกด้วย cURL ชิลๆ\n",
    "    download_task = BashOperator(\n",
    "        task_id=\"download_raw_data\",\n",
    "        bash_command=\"curl -sL https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv -o my_data_pipeline/raw_data/titanic.csv\"\n",
    "    )\n",
    "\n",
    "    # 📌 งานที่ 2: รันสคริปต์ Python ตัวเก่งของเรา (ต้องรอโหลกเสร็จนะไม่งั้นพัง!)\n",
    "    process_task = BashOperator(\n",
    "        task_id=\"clean_data_with_python\",\n",
    "        bash_command=\"python3 my_data_pipeline/clean_titanic.py\"\n",
    "    )\n",
    "\n",
    "    # 📌 งานที่ 3: ยกไฟล์ต้นฉบับไปเก็บที่ Archive จะได้ไม่รกโฟลเดอร์ (ต้องรอ Process เสร็จก่อน)\n",
    "    archive_task = BashOperator(\n",
    "        task_id=\"archive_raw_data\",\n",
    "        bash_command=\"mv my_data_pipeline/raw_data/titanic.csv my_data_pipeline/archive/titanic_$(date +%Y%m%d_%H%M%S).csv\"\n",
    "    )\n",
    "\n",
    "    # 🌟 พระเอกอยู่ตรงนี้: จัดเรียงคิวการทำงาน (Dependency Wiring)\n",
    "    # ถ้า download_task พัง หรือโหลดไม่เสร็จ -> process_task จะไม่เริ่มเด็ดขาด!\n",
    "    download_task >> process_task >> archive_task\n",
    "\n",
    "print(\"✅ สร้างไฟล์ Airflow DAG เรียบร้อยแล้ว!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌟 ขั้นตอนที่ 4: สิ่งที่จะเกิดขึ้นเมื่อ Airflow จัดการ Pipeline นี้\n",
    "\n",
    "1. **`download_raw_data` เริ่มทำงาน:** ยิง API โหลดไฟล์จาก Github \n",
    "   - ถ้าระหว่างโหลดเน็ตหลุด -> ระบบจะขึ้นสถานะ **Failed** สีแดง และพักรอ 1 นาที (ตามที่เราตั้ง `retries` ไว้) แล้วพยายามโหลดใหม่\n",
    "2. **`clean_data_with_python` รอจังหวะ:** งานจำศีลรอจนกว่ากล่องแรกจะขึ้นสถานะ **Success** สีเขียว\n",
    "3. พอไฟเขียวปุ๊บ ระบบสั่งรัน `python3 clean_titanic.py` ทันที ข้อมูลดิบถูกแปรสภาพเป็นข้อมูลพร้อมวิเคราะห์อยู่ใน `processed_data/`\n",
    "4. **`archive_raw_data` เก็บกวาด:** เมื่อ Python กรอกข้อมูลเสร็จ งานที่ 3 จะย้ายไฟล์ต้นฉบับไปเข้ากรุ `archive/` ป้องกันไม่ให้การรันในวันพรุ่งนี้มีไฟล์ทับซ้อนกัน\n",
    "\n",
    "และนี่แหละครับ คือพลังของ **\"Orchestration\"**! 🎉\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
